<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Robust Monocular Depth Estimation under Challenging Conditions, ICCV 2023">
  <meta name="keywords" content="depth estimation, challenging conditions, adverse weather, night, rain, snow, fog, self-supervised, monocular, supervised, adverse conditions, depth, ICCV 2023">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Robust Monocular Depth Estimation under Challenging Conditions</title>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYXS918KCT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PYXS918KCT');
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body" style="padding-top:6rem">
  <!-- style="background-color: rgb(240,240,240);"-->
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Robust Monocular Depth Estimation under Challenging Conditions</h1>

          <div class="is-size-4 publication-authors">
            <span class="author-block" style="padding-top:1.5rem; padding-bottom:1rem;"><b>ICCV 2023</b></span>
          </div>
          
          <div class="is-size-5 publication-authors" style="padding-bottom:1rem">
            <span class="author-block">
              <a href="https://www.cs.cit.tum.de/camp/members/stefano-gasperini/">Stefano Gasperini</a><sup>*,1,2</sup></span>
            <span class="author-block">
              <a>Nils Morbitzer</a><sup>*,1</sup></span>
            </span>
            <span class="author-block">
              <a href="https://www.cs.cit.tum.de/camp/members/hyunjun-jung/">HyunJun Jung</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://www.cs.cit.tum.de/camp/members/cv-nassir-navab/nassir-navab/">Nassir Navab</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://www.cs.cit.tum.de/camp/members/senior-research-scientists/federico-tombari/">Federico Tombari</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Technical University of Munich</span>
            <span class="author-block"><sup>2</sup> VisualAIs</span>
            <span class="author-block"><sup>3</sup> Google</span>
          </div>
          
          <div class="is-size-5 publication-authors" style="padding-bottom:1rem">
            <span class="author-block"><sup>*</sup> The authors contributed equally.</span>
          </div>
            
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!--span class="link-block">
                <a href="tbd_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>ICCV Paper</span>
                </a>
              </span-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.09711"
                   class="external-link button is-normal is-rounded is-dark" style="background-color: rgb(255, 214, 0);color: black;">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/md4all/md4all"
                   class="external-link button is-normal is-rounded is-dark" style="background-color: darkorange;color: black;">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="tbd_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video...</span>
                </a>
              </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://forms.gle/31w2TvtTiVNyPb916"
                   class="external-link button is-normal is-rounded is-dark" style="background-color: rgb(201, 0, 201);color: black;">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Translated Images</span>
                  </a>
                </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="padding-top:1rem;padding-bottom:4rem">
  <div class="container is-max-desktop" style="background-color: rgb(240,240,240);padding-top:4rem;padding-bottom:4rem">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-bottom:0.7rem">Can you see the trees?</h2>
        <div>
          <p>Hint: increase the brightness and look closely.</p>
        </div>
        <div class="">
          <img src="images/trees.png"></img>
        </div>
        <div>
          <p>Our monocular model outputs robust depth estimates in all conditions, even in the dark.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="padding-top:0">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!--div class="columns is-centered has-text-justified">
      <div class="column is-four-fifths" style="padding-bottom:3rem">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           abstract tbd
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <div class="columns is-centered has-text-justified">
      <div class="column is-four-fifths" style="padding-bottom:3rem">
        <h2 class="title is-3" style="margin-bottom:0">Motivation</h2>
        <div class="">
          <img src="images/factors.png"></img>
        </div>
        <p>
While state-of-the-art monocular depth estimation approaches achieve impressive results in ideal settings, they are highly unreliable under <b>challenging illumination and weather conditions</b>, such as at nighttime or in the presence of rain.
</br></br>
Noise, dark textureless areas, and reflections are detrimental factors that violate the training assumptions of both supervised and self-supervised methods.
While self-supervised works cannot establish the pixel correspondences needed to learn depth, 
supervised approaches learn artifacts from the ground truth sensor (LiDAR is shown in the figure above with samples from nuScenes).
        </p>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-four-fifths" style="padding-bottom:3rem">
        <h2 class="title is-3" style="margin-bottom:1rem">Method</h2>
        <div class="" style="margin-bottom:1rem">
          <img src="images/method.png"></img>
        </div>
        <p>
In this paper, we tackle these safety-critical issues with md4all: a simple and effective solution that works reliably under both adverse and ideal conditions, as well as for different types of learning supervision.
</br></br>
We achieve this by exploiting the efficacy of existing methods under perfect settings. Therefore, we provide <b>valid training signals independently of what is in the input</b>. First, with image translation, we generate a set of complex samples corresponding to the normal training ones. Then, we train the depth model by guiding its self- or full-supervision by feeding the generated samples and computing the standard losses on the corresponding original images.
</br></br>
As shown in the figure above, we take this further by distilling knowledge from a pre-trained baseline model, which infers only in ideal settings while feeding the depth model a mix of ideal and adverse inputs.
</br></br>
Our <a href="https://github.com/md4all/md4all">GitHub repository</a> contains the implementation code of the proposed techniques.
        </p>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-four-fifths" style="padding-bottom:3rem">
        <h2 class="title is-3" style="margin-bottom:0">Results</h2>
        <div class="">
          <img src="images/teaser.png"></img>
        </div>
        <p>
With md4all, we substantially outperform prior solutions delivering robust estimates in various conditions.
Remarkably, the proposed md4all uses <b>a single monocular model</b> and no specialized branches.
</br></br>
The figure above shows predictions in challenging settings of the nuScenes dataset. The self-supervised Monodepth2 cannot extract valuable features due to darkness and noise (top). The supervised AdaBins learns to mimic the sensor artifacts and predicts holes in the road (bottom). Applied on the same architectures, our md4all improves robustness in both standard and adverse conditions.
</br></br>
In our <a href="https://arxiv.org/abs/2308.09711">ICCV paper</a>, we demonstrate the <b>effectiveness of md4all in standard and adverse conditions, under both types of supervision</b>, via extensive experiments on two public datasets, namely nuScenes and Oxford RobotCar.
        </p>
      </div>
    </div>
        <div class="columns is-centered has-text-justified">
      <div class="column is-four-fifths" style="padding-bottom:3rem">
        <h2 class="title is-3" style="margin-bottom:1rem">Translated Images</h2>
        <div class="">
          <img src="images/translations.png"></img>
        </div>
        <p>
The figure above shows examples of the image translations generated to train our md4all. Our method acts as data augmentation by feeding the models with a mix of original and translated samples. This enables a single model to recover information across diverse conditions without modifications at inference time.
</br></br>
<a href="https://forms.gle/31w2TvtTiVNyPb916">Here</a>, we share open-source all images in adverse conditions generated to correspond to the sunny and cloudy samples of nuScenes and Oxford Robotcar training sets.
</br></br>
These images can be used for future robust methods for depth estimation or other tasks.
        </p>
      </div>
    </div>
<!--Qualitative results on nuScenes between the fully-supervised AdaBins without and with our approach, and the self-supervised Monodepth2 without and with our method.-->
    <!-- Paper video. -->
    <!--div class="columns is-centered has-text-justified">
      <div class="column is-four-fifths" style="padding-bottom:3rem">
        <h2 class="title is-3">Video</h2>
        <div class="">
          <!--iframe src="https://www.youtube.com/embed/LyugDwqfelQ?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe-->
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" style="padding-top:1.5rem;padding-bottom:6rem;" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre style="background-color: rgb(240,240,240);"><code>@inproceedings{gasperini_morbitzer2023md4all,
  title={Robust Monocular Depth Estimation under Challenging Conditions},
  author={Gasperini, Stefano and Morbitzer, Nils and Jung, HyunJun and Navab, Nassir and Tombari, Federico},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2023}
}
</code></pre>
  </div>
</section>


<footer class="footer" style="padding-bottom:3rem">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8 has-text-justified">
        <div class="content">
          <p>
            This website was adapted from <a href"https://nerfies.github.io">Nerfies</a> and is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            You can use the <a
              href="https://github.com/md4all/md4all.github.io">source code of this website</a>, we ask that you link back to this page in the footer. Please also remember to remove the analytics code in the website's header.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
